{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGnHw2bmgpGh","outputId":"4fe7e14c-6811-44c2-81d3-e2b6345f8e73"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#구글드라이브 연동\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","\n","data = pd.read_csv('/content/drive/MyDrive/추출요약_최종본_5분류.csv')"],"metadata":{"id":"lslQg1Qul3zh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.info()"],"metadata":{"id":"Mfe-uTM8RN_6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.drop(['Unnamed: 0'], axis=1, inplace=True)\n","data.drop(['Unnamed: 0.1'], axis=1, inplace=True)"],"metadata":{"id":"P6_XRJ64m0ri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.head()"],"metadata":{"id":"sEy1DoizRMXT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 📌 전체 데이터"],"metadata":{"id":"TxvFcUgLRRdt"}},{"cell_type":"code","source":["total_text = []\n","for text in data['번역']:\n","  total_text.append(text)"],"metadata":{"id":"kD-OFeAvm7nc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(total_text)"],"metadata":{"id":"9LEDnUt0yp24"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 📌 전체 데이터 프레임에 대하여 적용"],"metadata":{"id":"UKw6NWmBKSVw"}},{"cell_type":"code","source":["import re\n","\n","for i, text in enumerate(total_text):\n","  sentences = re.split(\"[\\.?!]\\S+\", text)\n","\n","  globals()[f'data{i}'] = []\n","\n","  for sentence in sentences:\n","    if (sentence == \"\" or len(sentence) == 0):\n","      continue\n","    temp_dict = dict()\n","    temp_dict['sentence'] = sentence\n","    temp_dict['token_list'] = sentence.split()\n","\n","    globals()[f'data{i}'].append(temp_dict)\n","\n","  globals()[f'df{i}'] = pd.DataFrame(globals()[f'data{i}'])\n","\n","# 마지막 데이터 확인\n","df7069"],"metadata":{"id":"kXqFV1WDRKxS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import normalize\n","\n","def pagerank(x, df=0.85, max_iter=30):\n","    assert 0 < df < 1\n","\n","    # initialize\n","    A = normalize(x, axis=0, norm='l1') \n","    R = np.ones(A.shape[0]).reshape(-1,1) \n","    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1) \n","\n","    # iteration\n","    for _ in range(max_iter): \n","        R = df * (A * R) + bias\n","\n","    return R"],"metadata":{"id":"nwLP-EOh40Zh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","import numpy as np\n","\n","df_list = [globals()[f'df{i}'] for i in range(7070)]\n","df_list = [data for data in df_list]\n","\n","total_summary_list = []\n","\n","for k, df in enumerate(df_list):\n","\n","  globals()[f'similarity_matrix{k}'] = []\n","\n","  for i, row_i in df.iterrows():\n","    i_row_vec = []\n","    for j, row_j in df.iterrows():\n","      if i == j:\n","        i_row_vec.append(0.0)\n","      else:\n","        try:\n","          intersection = len(set(row_i['token_list']) & set(row_j['token_list']))\n","          log_i = math.log(len(set(row_i['token_list'])))\n","          log_j = math.log(len(set(row_j['token_list'])))\n","          similarity = intersection / (log_i + log_j)\n","        except:\n","          pass\n","        i_row_vec.append(similarity)\n","    globals()[f'similarity_matrix{k}'].append(i_row_vec)\n","\n","\n","  weightedGraph = np.array(globals()[f'similarity_matrix{k}'])\n","\n","  R = pagerank(weightedGraph)\n","\n","  R = R.sum(axis=1) \n","  indexs = R.argsort()[-1:] # 해당 rank 값을 정렬, 값이 높은 1개의 문장 index를 반환\n","\n","  str_ = []\n","  for index in sorted(indexs):\n","      str_.append(df['sentence'][index])\n","  str_ = \" \".join(str_)\n","\n","  total_summary_list.append(str_)"],"metadata":{"id":"jvob3wNPyeSQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(total_summary_list))\n","print(total_summary_list[0])"],"metadata":{"id":"2scI2ITGRG2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['추출요약'] = total_summary_list"],"metadata":{"id":"ckvfS8OjHli-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.info()"],"metadata":{"id":"FB98G-0PREyZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.to_csv('/content/drive/MyDrive/추출요약_재요약.csv')"],"metadata":{"id":"Tyiduf2W42pp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 📌 [참고] 단계별 작동 방식 확인"],"metadata":{"id":"XfWUfieAiuUm"}},{"cell_type":"code","source":["text = \"\"\"관리 및 운영\n","음식물 쓰레기 줄이기\n","음식 쓰레기를 줄이는 것은 식품 안보의 중요한 측면이다. Queen's의 LeanPath Spark 프로그램은 공공 음식 저울과 디지털 사이니지를 사용하여 우리의 세 개의 주요 식당 중 두 곳의 행동 변화를 촉진하고 적극적으로 음식 낭비를 줄이는 것을 목표로 하고 있습니다. 주방에서는 이 기술을 통해 관리팀과 직원이\n","폐기물의 영향을 즉시 확인하고 그에 따라 조치를 취합니다. 식당에서는 이 프로그램이 소비자 후 쓰레기를 추적하고 음식 쓰레기를 줄이는 데 도움을 줌으로써 그들이 미칠 수 있는 영향에 대해 디지털 사이니지를 통해 식객들에게 교육한다. Queen's는 지속 가능하고 윤리적인 음식을 선택할 수 있는 Fair Trade 캠퍼스입니다.\n","우리의 공동체는 지속 가능하고 윤리적인 음식 선택에 접근할 수 있다.\n","교내에서 환대가 운영되는 모든 곳은 윤리적인 소싱을 우선시하며 공정 거래 옵션을 포함하도록 의무화되어 있습니다. 도널드 고든 센터는 2020년에 공정무역 직장 지정을 받았고, 2021년에는 농업 노동자와 환경을 위한 지속 가능한 관행을 지원하기 위해 노력하면서 공정무역 캠퍼스로 지정되었다.\"\"\""],"metadata":{"id":"nl_zoWUwk92i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# text를 문장 단위로 나누기\n","\n","import re\n"," \n","text = re.sub(r\"\\n+\", \" \", text) # 줄바꿈 기호(한 번 이상 반복) 제거\n","sentences = re.split(\"[\\.?!]\\s+\", text) # 구두점을 기준으로 문장 단위로 나누기\n","sentences"],"metadata":{"id":"JNcU4ta3RfkM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 각 문장을 요소 단위로 나누기 (토큰화)\n","\n","import pandas as pd \n","\n","data = []\n","\n","for sentence in sentences:\n","  if (sentence == \"\" or len(sentence) == 0): # 빈 문장인 경우 건너뛰기\n","    continue\n","  temp_dict = dict() # 빈 딕셔너리 생성 : 문장과 토큰을 한 딕셔너리로 묶기\n","  temp_dict['sentence'] = sentence # 문장\n","  temp_dict['token_list'] = sentence.split() # 토큰 : 띄어쓰기 단위로 토큰화\n","\n","  data.append(temp_dict)\n","\n","df = pd.DataFrame(data) # DataFrame에 넣어 깔끔하게 보기"],"metadata":{"id":"Kpqz8pgCliQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"6gj_UhNNRhaq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ✔️ 문장들 간의 유사도 계산\n","\n","> - 문장 간 유사도 = 두 문장에 동시에 등장하는 요소의 개수 / (log(문장A의 요소 개수) + log(문장B의 요소 개수))\n","> - sentence similarity = len(intersection) / (log(len(set_A)) + log(len(set_B)))\n","\n","-> 해당 수식을 사용했을 경우, 최대 값이 1이 아니고 문장의 길이가 길수록 높은 유사도를 갖게 된다. <br>\n","-> 코사인 유사도(Cosine Similarity)를 유사도 함수로 사용했을 경우, <br>짧은 문장에 민감하게 반응하여 2개의 단어를 가진 문장들이 1개의 단어만 겹쳐도 유사하다고 판단"],"metadata":{"id":"kVuDSWPOxphY"}},{"cell_type":"code","source":["# 문장들 간의 유사도 계산\n","\n","import math\n","\n","similarity_matrix = [] # 전체 문장들 간의 유사도를 저장할 리스트 선언\n","\n","for i, row_i in df.iterrows(): \n","  i_row_vec = [] # 타깃 문장과 나머지 문장들 간의 유사도를 저장할 리스트 선언\n","  for j, row_j in df.iterrows():\n","    if i == j: # 같은 문장인 경우 제외\n","      i_row_vec.append(0.0)\n","    else: # 다른 문장인 경우에 한해서 문장 간의 유사도 계산\n","      intersection = len(set(row_i['token_list']) & set(row_j['token_list'])) # 두 문장에 동시에 등장하는 토큰의 개수\n","      log_i = math.log(len(set(row_i['token_list']))) # log(문장A의 토큰의 개수)\n","      log_j = math.log(len(set(row_j['token_list']))) # log(문장B의 토큰의 개수)\n","      similarity = intersection / (log_i + log_j) # 문장 간 유사도 계산\n","      i_row_vec.append(similarity) # 두 문장 간 유사도 저장\n","  similarity_matrix.append(i_row_vec) # 전체 문장들 간의 유사도 저장"],"metadata":{"id":"zfdl6Xtml84O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 문장 유사도 그래프 생성\n","\n","import numpy as np\n","\n","weightedGraph = np.array(similarity_matrix)\n","weightedGraph"],"metadata":{"id":"9BfCxV94SFO6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 문장 유사도 그래프를 통해 문장의 rank를 계산해준다.\n","\n","# 문장 유사도 그래프에 PageRank를 학습하는 함수 생성\n","\n","from sklearn.preprocessing import normalize\n","\n","def pagerank(x, df=0.85, max_iter=30): # x : 문장 유사도 그래프 | df(damping factor) | max_iter : 반복횟수\n","    assert 0 < df < 1 # 가정 설정문\n","\n","    # initialize\n","    A = normalize(x, axis=0, norm='l1') # pagerank 알고리즘의 페이지 중요도를 다 더해서 1로 만들어주는 작업\n","    R = np.ones(A.shape[0]).reshape(-1,1) # R의 경우, 각 문장의 rank값을 의미하고 맨 처음은 1로 초기화되어 있음 \n","    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1) # bias의 경우, 만족하지 못하고 페이지를 떠나는 확률로 (1 - damping factor)를 의미\n","    \n","    # iteration : 각 rank는 weighted graph * rank*damping factor + (1-damping factor)로 계산한다.\n","    for _ in range(max_iter): \n","        R = df * (A * R) + bias\n","\n","    return R"],"metadata":{"id":"bEO_PpfTnQ-V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # normalize 예시\n","\n","# from sklearn.preprocessing import normalize\n","\n","# test_matrix = [\n","#     [1,0,1],\n","#     [1,1,0],\n","#     [1,1,1]\n","# ]\n","# test_matrix = np.array(test_matrix)\n","# print(normalize(test_matrix, axis=0, norm='l1'))"],"metadata":{"id":"ARw15hjSnjWT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 적용\n","R = pagerank(weightedGraph)"],"metadata":{"id":"DZui88VcpT-q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(type(R))"],"metadata":{"id":"LjrfcJQtSH4-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# R은 각 문장별 다른 문장으로부터 rank를 나눠 받은 상태\n","R"],"metadata":{"id":"57KfhkFISJXq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# row별로 sum을 하면 각 문장 당 최종 rank 값이 나옴\n","# rank 높은 순으로 문장을 뽑아 나열하면 추출요약이 됨\n","\n","R = pagerank(weightedGraph) # pagerank를 돌려서 rank matrix 반환\n","R = R.sum(axis=1) # 반환된 matrix를 row 별로 sum = 최종 rank 값이 나옴\n","indexs = R.argsort()[-3:] # 해당 rank 값을 정렬, 값이 높은 3개의 문장 index를 반환\n","print(indexs)\n","print(sorted(indexs))\n","\n","# rank값이 높은 문장을 프린트\n","for index in sorted(indexs): # index를 sorted 하는 이유 : 원래 문장 순서에 맞춰 보여주기 위함\n","    print(df['sentence'][index])"],"metadata":{"id":"eYs4S3YaSK91"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 📌 [추가] 문장 요소를 띄어쓰기가 아닌 형태소 분석기로 나누기"],"metadata":{"id":"CJcFzfJ9xLcA"}},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"id":"W35KdfrQSNON"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from konlpy.tag import Okt, Komoran, Kkma, Hannanum, Mecab\n","\n","okt = Okt()\n","komoran = Komoran()\n","kkma = Kkma()\n","hannanum = Hannanum()\n","mecab = Mecab()\n","\n","token_data_okt = []\n","token_data_komoran = []\n","token_data_kkma = []\n","token_data_hannanum = []\n","token_data_mecab = []\n","\n","for i, row in df.iterrows():\n","  sentence = row['sentence']\n","  # okt\n","  token_list_okt = komoran.nouns(sentence)\n","  token_data_okt.append(token_list_okt)\n","  # komoran\n","  token_list_komoran = komoran.nouns(sentence)\n","  token_data_komoran.append(token_list_komoran)\n","  # kkma\n","  token_list_kkma = komoran.nouns(sentence)\n","  token_data_kkma.append(token_list_kkma)\n","  # hannanum\n","  token_list_hannanum = komoran.nouns(sentence)\n","  token_data_hannanum.append(token_list_hannanum)\n","  # mecab\n","  token_list_mecab = komoran.nouns(sentence)\n","  token_data_mecab.append(token_list_mecab)"],"metadata":{"id":"Ahsw8UqUE7ip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['token_okt'] = token_data_okt\n","df['token_komoran'] = token_data_komoran\n","df['token_kkma'] = token_data_kkma\n","df['token_hannanum'] = token_data_hannanum\n","df['token_mecab'] = token_data_mecab"],"metadata":{"id":"aXOMerX_F9r3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df\n","\n","# 이후 문장들 간 유사도 계산 부분 반복"],"metadata":{"id":"lYuiGMMEGjSl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 📌 [추가] 문장추출요약 with Gensim_summarization"],"metadata":{"id":"-_CtfWzdH_25"}},{"cell_type":"code","source":["from gensim.summarization.summarizer import summarize"],"metadata":{"id":"J8_tpTmRzLAV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_hannanum = \"\"\"관리 및 운영\n","음식물 쓰레기 줄이기\n","음식 쓰레기를 줄이는 것은 식품 안보의 중요한 측면이다\n","Queen's의 LeanPath Spark 프로그램은 공공 음식 저울과 디지털 사이니지를 사용하여 우리의 세 개의 주요 식당 중 두 곳의 행동 변화를 촉진하고 적극적으로 음식 낭비를 줄이는 것을 목표로 하고 있습니다\n","주방에서는 이 기술을 통해 관리팀과 직원이\n","폐기물의 영향을 즉시 확인하고 그에 따라 조치를 취합니다\n","식당에서는 이 프로그램이 소비자 후 쓰레기를 추적하고 음식 쓰레기를 줄이는 데 도움을 줌으로써 그들이 미칠 수 있는 영향에 대해 디지털 사이니지를 통해 식객들에게 교육한다\n","Queen's는 지속 가능하고 윤리적인 음식을 선택할 수 있는 Fair Trade 캠퍼스입니다\n","우리의 공동체는 지속 가능하고 윤리적인 음식 선택에 접근할 수 있다\n","교내에서 환대가 운영되는 모든 곳은 윤리적인 소싱을 우선시하며 공정 거래 옵션을 포함하도록 의무화되어 있습니다\n","도널드 고든 센터는 2020년에 공정무역 직장 지정을 받았고, 2021년에는 농업 노동자와 환경을 위한 지속 가능한 관행을 지원하기 위해 노력하면서 공정무역 캠퍼스로 지정되었다.\"\"\""],"metadata":{"id":"qqNc5OD-fP7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(summarize(text_hannanum, ratio=0.1))"],"metadata":{"id":"rTgMFloNSQEs"},"execution_count":null,"outputs":[]}]}